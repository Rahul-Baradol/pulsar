## Tiny autograd inspired from Pytorch 

A auto-differentiation engine which makes it easy to build and experiment neural nets

So far,
- You can create neural nets of varying no. of layers, with different activation for each
- 2 activation functions supported, `tanh` and `sigmoid`
- 2 loss functions supported, `mean squared` and `binary cross entropy`

Check [experiments](https://github.com/Rahul-Baradol/pulsar/tree/main/experiments) where the tool is tested with real world examples

## Why build this?

For fun :)